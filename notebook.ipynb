{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b9463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fc0a30",
   "metadata": {},
   "source": [
    "# Utils functions used across the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcf91e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_checkpoints(checkpoint_dir: str = 'checkpoints') -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Discover all checkpoint files and parse metadata from filenames.\n",
    "\n",
    "    Expected filename format: {model}_{k}{K}_epochs{E}_seed{S}.pt\n",
    "    Examples:\n",
    "        - vae_k1_epochs50_seed42.pt\n",
    "        - iwae_k5_epochs50_seed42.pt\n",
    "        - iwae_k20_epochs50_seed42.pt\n",
    "\n",
    "    Returns:\n",
    "        List of dicts with keys: name, path, type, k, epochs, seed\n",
    "    \"\"\"\n",
    "    checkpoints = []\n",
    "    pattern = os.path.join(checkpoint_dir, '*.pt')\n",
    "\n",
    "    # Regex to parse checkpoint filenames\n",
    "    filename_regex = re.compile(\n",
    "        r'^(?P<model_type>vae|iwae)_k(?P<k>\\d+)_epochs(?P<epochs>\\d+)_seed(?P<seed>\\d+)\\.pt$'\n",
    "    )\n",
    "\n",
    "    for filepath in sorted(glob(pattern)):\n",
    "        filename = os.path.basename(filepath)\n",
    "        match = filename_regex.match(filename)\n",
    "\n",
    "        if match:\n",
    "            model_type = match.group('model_type')\n",
    "            k = int(match.group('k'))\n",
    "            epochs = int(match.group('epochs'))\n",
    "            seed = int(match.group('seed'))\n",
    "\n",
    "            # Create human-readable name\n",
    "            if model_type == 'vae':\n",
    "                name = f'VAE (K={k})'\n",
    "            else:\n",
    "                name = f'IWAE (K={k})'\n",
    "\n",
    "            checkpoints.append({\n",
    "                'name': name,\n",
    "                'path': filepath,\n",
    "                'type': model_type,\n",
    "                'k': k,\n",
    "                'epochs': epochs,\n",
    "                'seed': seed\n",
    "            })\n",
    "        else:\n",
    "            # Non-standard filename, include with minimal info\n",
    "            print(f\"Warning: Could not parse checkpoint filename: {filename}\")\n",
    "            checkpoints.append({\n",
    "                'name': filename.replace('.pt', ''),\n",
    "                'path': filepath,\n",
    "                'type': 'unknown',\n",
    "                'k': None,\n",
    "                'epochs': None,\n",
    "                'seed': None\n",
    "            })\n",
    "\n",
    "    return checkpoints\n",
    "\n",
    "\n",
    "def get_model_key(checkpoint_path: str) -> str:\n",
    "    \"\"\"Get a unique key for a model from its checkpoint path.\"\"\"\n",
    "    return os.path.basename(checkpoint_path).replace('.pt', '')\n",
    "\n",
    "\n",
    "def load_results(results_path: str = 'results/evaluations.yaml') -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load evaluation results from YAML file.\n",
    "\n",
    "    Returns:\n",
    "        Dict mapping model keys to their evaluation metrics\n",
    "    \"\"\"\n",
    "    if not os.path.exists(results_path):\n",
    "        return {}\n",
    "\n",
    "    with open(results_path, 'r') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "\n",
    "    return data if data else {}\n",
    "\n",
    "\n",
    "def _convert_to_native(obj: Any) -> Any:\n",
    "    \"\"\"\n",
    "    Recursively convert numpy types to native Python types for YAML serialization.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: _convert_to_native(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [_convert_to_native(v) for v in obj]\n",
    "    elif isinstance(obj, (np.integer,)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating,)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "def save_results(\n",
    "    results_path: str,\n",
    "    model_key: str,\n",
    "    metrics: Dict[str, Any],\n",
    "    model_info: Optional[Dict[str, Any]] = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Save or update evaluation results for a model in the YAML file.\n",
    "\n",
    "    Args:\n",
    "        results_path: Path to the YAML file\n",
    "        model_key: Unique identifier for the model (typically filename without .pt)\n",
    "        metrics: Dict of metric names to values (e.g., {'log_likelihood': -81.06})\n",
    "        model_info: Optional dict with model metadata (type, k, path, etc.)\n",
    "    \"\"\"\n",
    "    # Load existing results\n",
    "    results = load_results(results_path)\n",
    "\n",
    "    # Initialize entry if doesn't exist\n",
    "    if model_key not in results:\n",
    "        results[model_key] = {}\n",
    "\n",
    "    # Update with model info if provided (convert numpy types)\n",
    "    if model_info:\n",
    "        results[model_key].update(_convert_to_native(model_info))\n",
    "\n",
    "    # Update with new metrics (convert numpy types)\n",
    "    results[model_key].update(_convert_to_native(metrics))\n",
    "\n",
    "    # Ensure directory exists\n",
    "    os.makedirs(os.path.dirname(results_path), exist_ok=True)\n",
    "\n",
    "    # Save back to YAML\n",
    "    with open(results_path, 'w') as f:\n",
    "        yaml.dump(results, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "\n",
    "def get_models_config(\n",
    "    checkpoint_dir: str = 'checkpoints',\n",
    "    results_path: str = 'results/evaluations.yaml'\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Get full model configurations by merging checkpoint discovery with stored results.\n",
    "\n",
    "    This is the main function that analysis scripts should use to get model configs\n",
    "    instead of hardcoding them.\n",
    "\n",
    "    Returns:\n",
    "        List of model configs with all available metrics\n",
    "    \"\"\"\n",
    "    # Discover checkpoints\n",
    "    checkpoints = discover_checkpoints(checkpoint_dir)\n",
    "\n",
    "    # Load stored results\n",
    "    results = load_results(results_path)\n",
    "\n",
    "    # Merge checkpoint info with stored results\n",
    "    models_config = []\n",
    "    for checkpoint in checkpoints:\n",
    "        model_key = get_model_key(checkpoint['path'])\n",
    "        config = checkpoint.copy()\n",
    "\n",
    "        # Add stored metrics if available\n",
    "        if model_key in results:\n",
    "            stored = results[model_key]\n",
    "            # Add metrics that aren't already in config\n",
    "            for key, value in stored.items():\n",
    "                if key not in config:\n",
    "                    config[key] = value\n",
    "\n",
    "        models_config.append(config)\n",
    "\n",
    "    return models_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a182606",
   "metadata": {},
   "source": [
    "# Functions for Model Analysis and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601c8533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_variance(model, data, n_runs=100):\n",
    "    \"\"\"\n",
    "    Computes the variance of the gradients for the encoder parameters\n",
    "    over multiple runs on the SAME batch of data (varying random sampling).\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    # Target the first encoder layer\n",
    "    target_layer_param = model.encoder[0].weight\n",
    "    grads = []\n",
    "\n",
    "    for _ in range(n_runs):\n",
    "        model.zero_grad()\n",
    "        recon_x, mu, logvar, z = model(data)\n",
    "        loss = model.compute_loss(data, recon_x, mu, logvar, z)\n",
    "        loss.backward()\n",
    "\n",
    "        if target_layer_param.grad is not None:\n",
    "            grads.append(target_layer_param.grad.clone().cpu().numpy())\n",
    "\n",
    "    grads = np.array(grads)\n",
    "\n",
    "    var_per_param = np.var(grads, axis=0)\n",
    "    mean_grad = np.mean(grads, axis=0)\n",
    "    avg_variance = np.mean(var_per_param)\n",
    "\n",
    "    std_per_param = np.std(grads, axis=0) + 1e-10\n",
    "    snr = np.mean(np.abs(mean_grad) / std_per_param)\n",
    "\n",
    "    return avg_variance, snr\n",
    "\n",
    "\n",
    "def analyze_single_checkpoint(\n",
    "    checkpoint_path: str,\n",
    "    model_type: str,\n",
    "    k: int,\n",
    "    device: str,\n",
    "    data: torch.Tensor,\n",
    "    n_runs: int = 50,\n",
    "    results_path: str = None,\n",
    "    hidden_size: int = 200,\n",
    "    latent_size: int = 50\n",
    ") -> tuple:\n",
    "    \"\"\"Analyze gradient variance for a single checkpoint.\"\"\"\n",
    "    input_size = 784\n",
    "    output_size = 784\n",
    "\n",
    "    if model_type == 'vae':\n",
    "        model = VAE(input_size, hidden_size, latent_size, output_size)\n",
    "    else:\n",
    "        model = IWAE(k, input_size, hidden_size, latent_size, output_size)\n",
    "\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    model.to(device)\n",
    "\n",
    "    variance, snr = compute_gradient_variance(model, data, n_runs=n_runs)\n",
    "\n",
    "    # Save to YAML if path provided\n",
    "    if results_path:\n",
    "        model_key = get_model_key(checkpoint_path)\n",
    "        save_results(\n",
    "            results_path=results_path,\n",
    "            model_key=model_key,\n",
    "            metrics={\n",
    "                'gradient_variance': float(f\"{variance:.2e}\"),\n",
    "                'gradient_snr': round(snr, 4)\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return variance, snr\n",
    "\n",
    "\n",
    "def analyze_all_checkpoints(\n",
    "    checkpoint_dir: str,\n",
    "    device: str,\n",
    "    data: torch.Tensor,\n",
    "    n_runs: int = 50,\n",
    "    results_path: str = 'results/evaluations.yaml'\n",
    "):\n",
    "    \"\"\"Discover and analyze all checkpoints.\"\"\"\n",
    "    checkpoints = discover_checkpoints(checkpoint_dir)\n",
    "\n",
    "    if not checkpoints:\n",
    "        print(f\"No checkpoints found in {checkpoint_dir}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(checkpoints)} checkpoints\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    results = []\n",
    "    for cp in checkpoints:\n",
    "        if cp['k'] is None:\n",
    "            print(f\"Skipping {cp['name']} - could not parse K value\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nAnalyzing {cp['name']}...\")\n",
    "\n",
    "        variance, snr = analyze_single_checkpoint(\n",
    "            checkpoint_path=cp['path'],\n",
    "            model_type=cp['type'],\n",
    "            k=cp['k'],\n",
    "            device=device,\n",
    "            data=data,\n",
    "            n_runs=n_runs,\n",
    "            results_path=results_path\n",
    "        )\n",
    "\n",
    "        results.append((cp['name'], variance, snr))\n",
    "        print(f\"  Variance: {variance:.2e}, SNR: {snr:.4f}\")\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(\"\\nSummary:\")\n",
    "    print(f\"{'Model':<20} | {'Variance':<12} | {'SNR':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    for name, var, snr in results:\n",
    "        print(f\"{name:<20} | {var:<12.2e} | {snr:<10.4f}\")\n",
    "\n",
    "    if results_path:\n",
    "        print(f\"\\nResults saved to {results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc10336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_dataloaders(batch_size=128, data_dir='./data'):\n",
    "    \"\"\"\n",
    "    Returns (train_loader, val_loader, test_loader) for MNIST.\n",
    "\n",
    "    Preprocessing:\n",
    "    - Train: Dynamic Binarization (sampling from pixel intensities)\n",
    "    - Val/Test: Fixed Binarization (rounding at 0.5)\n",
    "\n",
    "    Split:\n",
    "    - Train: 50,000 samples\n",
    "    - Val: 10,000 samples (from the original training set)\n",
    "    - Test: 10,000 samples (original test set)\n",
    "    \"\"\"\n",
    "\n",
    "    # Transforms\n",
    "    # Dynamic binarization: interpret pixel value as probability p, sample x ~ Bern(p)\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: torch.bernoulli(x))\n",
    "    ])\n",
    "\n",
    "    # Fixed binarization: round to nearest integer (0 or 1)\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: torch.round(x))\n",
    "    ])\n",
    "\n",
    "    # Download and load datasets\n",
    "    # We load train dataset twice to apply different transforms for train vs val\n",
    "    full_train_dynamic = datasets.MNIST(data_dir, train=True, download=True, transform=train_transform)\n",
    "    full_train_fixed = datasets.MNIST(data_dir, train=True, download=True, transform=test_transform)\n",
    "    test_dataset = datasets.MNIST(data_dir, train=False, download=True, transform=test_transform)\n",
    "\n",
    "    # Create Split Indices (fixed seed for reproducibility of split)\n",
    "    generator = torch.Generator().manual_seed(42)\n",
    "    indices = torch.randperm(len(full_train_dynamic), generator=generator)\n",
    "\n",
    "    train_indices = indices[:50000]\n",
    "    val_indices = indices[50000:]\n",
    "\n",
    "    # Create Subsets\n",
    "    train_dataset = Subset(full_train_dynamic, train_indices)\n",
    "    val_dataset = Subset(full_train_fixed, val_indices)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(f\"Data Loaded: Train {len(train_dataset)}, Val {len(val_dataset)}, Test {len(test_dataset)}\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e769e4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstruction(model, dataloader, device, save_path):\n",
    "    model.eval()\n",
    "    data, _ = next(iter(dataloader))\n",
    "    data = data.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        recon, _, _, _ = model(data)\n",
    "        # If IWAE, recon is (K, B, 784), take mean\n",
    "        if len(recon.shape) == 3:\n",
    "            recon = recon.mean(dim=0)\n",
    "\n",
    "    # Randomly select 8 samples\n",
    "    batch_size = data.size(0)\n",
    "    num_samples = min(8, batch_size)\n",
    "    indices = torch.randperm(batch_size)[:num_samples]\n",
    "    \n",
    "    # Reshape\n",
    "    input_imgs = data[indices].view(-1, 28, 28).cpu()\n",
    "    recon_imgs = recon[indices].view(-1, 28, 28).cpu()\n",
    "\n",
    "    fig, axes = plt.subplots(2, 8, figsize=(12, 3))\n",
    "    for i in range(8):\n",
    "        # Original\n",
    "        axes[0, i].imshow(input_imgs[i], cmap='gray')\n",
    "        axes[0, i].axis('off')\n",
    "        if i == 0: axes[0, i].set_title(\"Original\")\n",
    "\n",
    "        # Recon\n",
    "        axes[1, i].imshow(recon_imgs[i], cmap='gray')\n",
    "        axes[1, i].axis('off')\n",
    "        if i == 0: axes[1, i].set_title(\"Recon\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"Reconstructions saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_samples(model, device, save_path, n=64):\n",
    "    model.eval()\n",
    "    # Sample from prior p(z) ~ N(0, I)\n",
    "    z = torch.randn(n, model.latent_size).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        samples = model.decoder(z)\n",
    "\n",
    "    samples = samples.view(-1, 28, 28).cpu()\n",
    "\n",
    "    # Grid size ( sqrt(n) )\n",
    "    grid_size = int(n**0.5)\n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(8, 8))\n",
    "\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        ax.imshow(samples[i], cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"Samples saved to {save_path}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea26ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_batch_kl(mu, logvar):\n",
    "    \"\"\"\n",
    "    Computes individual KL divergence for each dimension z_j.\n",
    "    Formula: KL = -0.5 * (1 + logvar - mu^2 - exp(logvar))\n",
    "\n",
    "    Args:\n",
    "        mu: (BATCH_SIZE, LATENT_SIZE)\n",
    "        logvar: (BATCH_SIZE, LATENT_SIZE)\n",
    "\n",
    "    Returns:\n",
    "        kl_per_dim: (LATENT_SIZE,) - Average KL for each dimension across the batch\n",
    "    \"\"\"\n",
    "    kl_elementwise = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    kl_per_dim = kl_elementwise.mean(dim=0)\n",
    "    return kl_per_dim\n",
    "\n",
    "\n",
    "def calc_active_units(model, dataloader, device, threshold=0.01):\n",
    "    \"\"\"\n",
    "    Runs over the dataset and computes the number of active units.\n",
    "    A unit is active if Avg_KL(z_j) > threshold.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_kl = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, _ in tqdm(dataloader, desc=\"Checking Latents\", leave=False):\n",
    "            data = data.to(device)\n",
    "\n",
    "            if isinstance(model, IWAE):\n",
    "                x_flat = data.view(data.size(0), -1)\n",
    "                h = model.encoder(x_flat)\n",
    "                mu, logvar = h.chunk(2, dim=1)\n",
    "            else:\n",
    "                _, mu, logvar, _ = model(data)\n",
    "\n",
    "            batch_kl = compute_batch_kl(mu, logvar)\n",
    "            total_kl += batch_kl\n",
    "            num_batches += 1\n",
    "\n",
    "    final_avg_kl = total_kl / num_batches\n",
    "    num_active = (final_avg_kl > threshold).sum().item()\n",
    "\n",
    "    return num_active, final_avg_kl\n",
    "\n",
    "\n",
    "def plot_kl_stats(avg_kl_values, model_name, save_path):\n",
    "    \"\"\"Plots the KL value for each dimension sorted.\"\"\"\n",
    "    sorted_kl, _ = torch.sort(avg_kl_values, descending=True)\n",
    "    sorted_kl = sorted_kl.cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.bar(range(len(sorted_kl)), sorted_kl)\n",
    "    plt.axhline(y=0.01, color='r', linestyle='--', label='Threshold (0.01)')\n",
    "    plt.xlabel('Latent Dimensions (Sorted)')\n",
    "    plt.ylabel('Average KL Divergence (nats)')\n",
    "    plt.title(f'Effective KL per Dimension - {model_name}')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"Plot saved to {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def analyze_single_checkpoint(\n",
    "    checkpoint_path: str,\n",
    "    model_type: str,\n",
    "    k: int,\n",
    "    device: str,\n",
    "    test_loader,\n",
    "    threshold: float = 0.01,\n",
    "    output_dir: str = './results',\n",
    "    results_path: str = None,\n",
    "    hidden_size: int = 200,\n",
    "    latent_size: int = 50\n",
    ") -> int:\n",
    "    \"\"\"Analyze a single checkpoint and optionally save results.\"\"\"\n",
    "    input_size = 784\n",
    "    output_size = 784\n",
    "\n",
    "    if model_type == 'vae':\n",
    "        model = VAE(input_size, hidden_size, latent_size, output_size)\n",
    "    else:\n",
    "        model = IWAE(k, input_size, hidden_size, latent_size, output_size)\n",
    "\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    model.to(device)\n",
    "\n",
    "    n_active, avg_kls = calc_active_units(model, test_loader, device, threshold)\n",
    "\n",
    "    # Save to YAML if path provided\n",
    "    if results_path:\n",
    "        model_key = get_model_key(checkpoint_path)\n",
    "        save_results(\n",
    "            results_path=results_path,\n",
    "            model_key=model_key,\n",
    "            metrics={'active_units': n_active}\n",
    "        )\n",
    "\n",
    "    # Plot\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    base_name = os.path.basename(checkpoint_path).replace('.pt', '')\n",
    "    plot_kl_stats(\n",
    "        avg_kls,\n",
    "        f\"{model_type.upper()} (Active: {n_active})\",\n",
    "        os.path.join(output_dir, f\"{base_name}_kl.png\")\n",
    "    )\n",
    "\n",
    "    return n_active\n",
    "\n",
    "\n",
    "def analyze_all_checkpoints(\n",
    "    checkpoint_dir: str,\n",
    "    device: str,\n",
    "    test_loader,\n",
    "    threshold: float = 0.01,\n",
    "    output_dir: str = './results',\n",
    "    results_path: str = 'results/evaluations.yaml'\n",
    "):\n",
    "    \"\"\"Discover and analyze all checkpoints.\"\"\"\n",
    "    checkpoints = discover_checkpoints(checkpoint_dir)\n",
    "\n",
    "    if not checkpoints:\n",
    "        print(f\"No checkpoints found in {checkpoint_dir}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(checkpoints)} checkpoints\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    results = []\n",
    "    for cp in checkpoints:\n",
    "        if cp['k'] is None:\n",
    "            print(f\"Skipping {cp['name']} - could not parse K value\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nAnalyzing {cp['name']}...\")\n",
    "\n",
    "        n_active = analyze_single_checkpoint(\n",
    "            checkpoint_path=cp['path'],\n",
    "            model_type=cp['type'],\n",
    "            k=cp['k'],\n",
    "            device=device,\n",
    "            test_loader=test_loader,\n",
    "            threshold=threshold,\n",
    "            output_dir=output_dir,\n",
    "            results_path=results_path\n",
    "        )\n",
    "\n",
    "        results.append((cp['name'], n_active))\n",
    "        print(f\"  Active Units: {n_active}/50\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(\"\\nSummary:\")\n",
    "    for name, n_active in results:\n",
    "        print(f\"  {name}: {n_active} active units\")\n",
    "\n",
    "    if results_path:\n",
    "        print(f\"\\nResults saved to {results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7d9dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device, k_eval):\n",
    "    \"\"\"\n",
    "    Evaluates model using IWAE bound with k_eval samples.\n",
    "    Returns average Log-Likelihood (nats).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_ll = 0\n",
    "    total_recon_error = 0\n",
    "    total_weighted_recon_error = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Check if model has 'K' attribute and update it temporarily\n",
    "    original_k = getattr(model, 'K', 1)\n",
    "    if hasattr(model, 'K'):\n",
    "        model.K = k_eval\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, _ in tqdm(dataloader, desc=f\"Evaluating with K={k_eval}\", leave=False):\n",
    "            data = data.to(device)\n",
    "            recon_x, mu, logvar, z = model(data)\n",
    "            loss = model.compute_loss(data, recon_x, mu, logvar, z)\n",
    "\n",
    "            # Calculate Reconstruction Error (BCE)\n",
    "            # Shape of recon_x: (K, B, L) for IWAE/VAE(wrapped), or (B, L) for VAE(native)\n",
    "            # We standardize to (K, B, L) for computation\n",
    "            x_flat = data.view(data.size(0), -1)\n",
    "\n",
    "            if len(recon_x.shape) == 2: # (B, L)\n",
    "                 # VAE standard\n",
    "                 bce = torch.nn.functional.binary_cross_entropy(recon_x, x_flat, reduction='sum')\n",
    "                 recon_error = bce.item()\n",
    "                 weighted_recon_error = recon_error\n",
    "            else: # (K, B, L)\n",
    "                 # IWAE: Compute expected reconstruction error over K samples\n",
    "                 # Sum over Batch & Latent, Mean over K\n",
    "                 x_expanded = x_flat.unsqueeze(0).expand_as(recon_x) # (K, B, L)\n",
    "                 # BCE per sample per batch: (K, B)\n",
    "                 bce_per_sample = torch.nn.functional.binary_cross_entropy(recon_x, x_expanded, reduction='none').sum(dim=2)\n",
    "\n",
    "                 # 1. Unweighted Reconstruction Error (Expected Value)\n",
    "                 recon_error = bce_per_sample.mean(dim=0).sum().item()\n",
    "\n",
    "                 # 2. Weighted Reconstruction Error (Importance Sampled)\n",
    "                 log_p_x_given_z = -bce_per_sample # (K, B)\n",
    "\n",
    "                 # q(z|x) and p(z)\n",
    "                 if mu.dim() == 2:\n",
    "                     mu_k = mu.unsqueeze(0).expand_as(z)\n",
    "                     logvar_k = logvar.unsqueeze(0).expand_as(z)\n",
    "                 else:\n",
    "                     mu_k, logvar_k = mu, logvar\n",
    "\n",
    "                 log_q_z_given_x = -0.5 * (torch.log(2 * torch.tensor(math.pi)) + logvar_k + (z - mu_k).pow(2) / torch.exp(logvar_k))\n",
    "                 log_q_z_given_x = log_q_z_given_x.sum(dim=2) # (K, B)\n",
    "\n",
    "                 log_p_z = -0.5 * (torch.log(2 * torch.tensor(math.pi)) + z.pow(2))\n",
    "                 log_p_z = log_p_z.sum(dim=2) # (K, B)\n",
    "\n",
    "                 log_w = log_p_x_given_z + log_p_z - log_q_z_given_x # (K, B)\n",
    "                 w_tilde = torch.softmax(log_w, dim=0) # (K, B)\n",
    "\n",
    "                 weighted_bce = (w_tilde * bce_per_sample).sum(dim=0) # (B,)\n",
    "                 weighted_recon_error = weighted_bce.sum().item()\n",
    "\n",
    "            batch_img_count = data.size(0)\n",
    "            total_ll += -loss.item() * batch_img_count\n",
    "            total_samples += batch_img_count\n",
    "            total_recon_error += recon_error\n",
    "            total_weighted_recon_error += weighted_recon_error\n",
    "\n",
    "    # Restore K just in case\n",
    "    if hasattr(model, 'K'):\n",
    "        model.K = original_k\n",
    "\n",
    "    return total_ll / total_samples, total_recon_error / total_samples, total_weighted_recon_error / total_samples\n",
    "\n",
    "\n",
    "def evaluate_single_checkpoint(\n",
    "    checkpoint_path: str,\n",
    "    model_type: str,\n",
    "    k_train: int,\n",
    "    k_eval: int,\n",
    "    device: str,\n",
    "    test_loader,\n",
    "    hidden_size: int = 200,\n",
    "    latent_size: int = 50,\n",
    "    results_path: str = None\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Evaluate a single checkpoint and optionally save results.\n",
    "\n",
    "    Returns:\n",
    "        Log-likelihood estimate\n",
    "    \"\"\"\n",
    "    input_size = 784\n",
    "    output_size = 784\n",
    "\n",
    "    # Load weights into IWAE evaluator (works for both VAE and IWAE checkpoints)\n",
    "    evaluator = IWAE(\n",
    "        K=k_eval,\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        latent_size=latent_size,\n",
    "        output_size=output_size\n",
    "    )\n",
    "    evaluator.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    evaluator.to(device)\n",
    "\n",
    "    ll, recon_error, weighted_recon_error = evaluate_model(evaluator, test_loader, device, k_eval)\n",
    "\n",
    "    # Save to YAML if path provided\n",
    "    if results_path:\n",
    "        model_key = get_model_key(checkpoint_path)\n",
    "\n",
    "        metrics = {\n",
    "            'log_likelihood': round(ll, 2),\n",
    "            'reconstruction_error': round(recon_error, 2)\n",
    "        }\n",
    "\n",
    "        # Add weighted recon error only for IWAE (K>1)\n",
    "        if model_type == 'iwae' and k_train > 1:\n",
    "            metrics['weighted_reconstruction_error'] = round(weighted_recon_error, 2)\n",
    "\n",
    "        save_results(\n",
    "            results_path=results_path,\n",
    "            model_key=model_key,\n",
    "            metrics=metrics,\n",
    "            model_info={\n",
    "                'path': checkpoint_path,\n",
    "                'type': model_type,\n",
    "                'k': k_train\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return ll, recon_error, weighted_recon_error\n",
    "\n",
    "\n",
    "def evaluate_all_checkpoints(\n",
    "    checkpoint_dir: str,\n",
    "    k_eval: int,\n",
    "    device: str,\n",
    "    test_loader,\n",
    "    hidden_size: int = 200,\n",
    "    latent_size: int = 50,\n",
    "    results_path: str = 'results/evaluations.yaml'\n",
    "):\n",
    "    \"\"\"\n",
    "    Discover and evaluate all checkpoints in directory.\n",
    "    \"\"\"\n",
    "    checkpoints = discover_checkpoints(checkpoint_dir)\n",
    "\n",
    "    if not checkpoints:\n",
    "        print(f\"No checkpoints found in {checkpoint_dir}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(checkpoints)} checkpoints\")\n",
    "    print(f\"Evaluation on Test Set (10k images) using IWAE bound with K={k_eval}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    results = []\n",
    "    for cp in checkpoints:\n",
    "        print(f\"\\nEvaluating {cp['name']}...\")\n",
    "\n",
    "        ll, recon_error, weighted_recon = evaluate_single_checkpoint(\n",
    "            checkpoint_path=cp['path'],\n",
    "            model_type=cp['type'],\n",
    "            k_train=cp['k'] if cp['k'] else 1,\n",
    "            k_eval=k_eval,\n",
    "            device=device,\n",
    "            test_loader=test_loader,\n",
    "            hidden_size=hidden_size,\n",
    "            latent_size=latent_size,\n",
    "            results_path=results_path\n",
    "        )\n",
    "\n",
    "        results.append((cp['name'], ll, recon_error, weighted_recon, cp.get('type'), cp.get('k')))\n",
    "\n",
    "        if cp.get('type') == 'iwae' and (cp.get('k') or 0) > 1:\n",
    "            print(f\"  {cp['name']} Log-Likelihood: {ll:.2f} nats | Recon: {recon_error:.2f} | Weighted Recon: {weighted_recon:.2f}\")\n",
    "        else:\n",
    "            print(f\"  {cp['name']} Log-Likelihood: {ll:.2f} nats | Recon: {recon_error:.2f}\")\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(\"\\nSummary:\")\n",
    "    # Sort by Log Likelihood\n",
    "    for name, ll, recon, weighted, mtype, k in sorted(results, key=lambda x: x[1], reverse=True):\n",
    "        if mtype == 'iwae' and k and k > 1:\n",
    "            print(f\"  {name:<15}: LL={ll:.2f} | Recon={recon:.2f} | Weighted={weighted:.2f}\")\n",
    "        else:\n",
    "            print(f\"  {name:<15}: LL={ll:.2f} | Recon={recon:.2f}\")\n",
    "\n",
    "    print(f\"\\nResults saved to {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a79fed4",
   "metadata": {},
   "source": [
    "# IWAE & VAE Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3418904",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_size, output_size):\n",
    "        \"\"\"\n",
    "        Initialise l'architecture du VAE.\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): Dimension de l'entrée (ex: 784 pour MNIST 28x28).\n",
    "            hidden_size (int): Nombre de neurones dans les couches cachées.\n",
    "            latent_size (int): Dimension de l'espace latent (le 'goulot d'étranglement').\n",
    "            output_size (int): Dimension de la sortie reconstruite (généralement = input_size).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- ENCODEUR ---\n",
    "        # Compresse l'entrée vers l'espace latent\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=input_size, out_features=hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Tanh(),\n",
    "            # La sortie finale de l'encodeur est 2 * latent_size car on doit\n",
    "            # prédire à la fois la moyenne (mu) et le log-variance (logvar).\n",
    "            nn.Linear(in_features=hidden_size, out_features=2 * latent_size)\n",
    "        )\n",
    "\n",
    "        # --- DÉCODEUR ---\n",
    "        # Reconstruit l'image à partir d'un point de l'espace latent\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=latent_size, out_features=hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=hidden_size, out_features=output_size),\n",
    "            # Sigmoid assure que la sortie est entre 0 et 1 (idéal pour des pixels normalisés).\n",
    "            nn.Sigmoid() \n",
    "        )\n",
    "\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Passage vers l'avant (Forward pass).\n",
    "        \"\"\"\n",
    "        # Aplatit l'entrée au cas où elle arrive sous forme d'image (B, C, H, W) -> (B, D)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # 1. ENCODAGE : Extraction des paramètres de la distribution gaussienne\n",
    "        h = self.encoder(x)\n",
    "        # Sépare le vecteur en deux : mu (moyenne) et logvar (log-variance)\n",
    "        mu, logvar = h.chunk(2, dim=1)\n",
    "\n",
    "        # 2. REPARAMÉTRISATION (The Reparameterization Trick)\n",
    "        # On ne peut pas faire de backpropagation à travers un échantillonnage aléatoire.\n",
    "        # On utilise donc : z = mu + sigma * epsilon, où epsilon ~ N(0, 1)\n",
    "        std = torch.exp(0.5 * logvar) # Calcul de l'écart-type (sigma)\n",
    "        eps = torch.randn_like(std)   # Échantillonnage d'un bruit blanc\n",
    "        z = mu + std * eps            # Échantillonnage de l'espace latent\n",
    "\n",
    "        # 3. DÉCODAGE : Reconstruction à partir de z\n",
    "        recon_x = self.decoder(z)\n",
    "        \n",
    "        return recon_x, mu, logvar, z\n",
    "\n",
    "    def compute_loss(self, x, recon_x, mu, logvar):\n",
    "        \"\"\"\n",
    "        Calcule la perte ELBO (Evidence Lower Bound).\n",
    "        Loss = Erreur de reconstruction + Divergence KL\n",
    "        \"\"\"\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # A. Perte de Reconstruction (BCE) : Compare l'original et la copie\n",
    "        # On utilise la somme (reduction='sum') pour rester cohérent avec la formule mathématique de la KL.\n",
    "        bce = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "        # B. Divergence de Kullback-Leibler (KLD)\n",
    "        # Force la distribution latente apprise à être proche d'une distribution normale standard N(0,1).\n",
    "        # Formule fermée : -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        return bce + kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558acbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IWAE(VAE):\n",
    "    def __init__(self, K, input_size, hidden_size, latent_size, output_size):\n",
    "        \"\"\"\n",
    "        Initialise l'IWAE.\n",
    "        Args:\n",
    "            K (int): Nombre d'échantillons d'importance par donnée.\n",
    "        \"\"\"\n",
    "        super().__init__(input_size=input_size, hidden_size=hidden_size, \n",
    "                         latent_size=latent_size, output_size=output_size)\n",
    "        self.K = K\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Passage vers l'avant avec échantillonnage multiple.\n",
    "        \"\"\"\n",
    "        # Aplatissement de l'entrée : (Batch, ...) -> (Batch, Input_size)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Encodage pour obtenir les paramètres de la distribution latente\n",
    "        h = self.encoder(x)\n",
    "        mu, logvar = h.chunk(2, dim=1) # Dimensions : (Batch, Latent)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        \n",
    "        # --- ÉCHANTILLONNAGE MULTIPLE (K échantillons) ---\n",
    "        # On génère K bruits blancs pour chaque élément du batch.\n",
    "        # Dimension finale de eps : (K, Batch, Latent)\n",
    "        eps = torch.randn(self.K, x.size(0), self.latent_size).to(x.device)\n",
    "\n",
    "        # Reparamétrisation : z a maintenant une dimension (K, Batch, Latent)\n",
    "        z = mu + eps * std\n",
    "\n",
    "        # --- DÉCODAGE MASSIVE ---\n",
    "        # On aplatit z pour passer tous les échantillons (K * Batch) dans le décodeur d'un coup\n",
    "        z_flat = z.view(-1, self.latent_size)\n",
    "        recon_x_flat = self.decoder(z_flat)\n",
    "        \n",
    "        # On redonne à la reconstruction sa forme multi-échantillons : (K, Batch, Output_size)\n",
    "        recon_x = recon_x_flat.view(self.K, x.size(0), -1)\n",
    "\n",
    "        return recon_x, mu, logvar, z\n",
    "\n",
    "    def compute_loss(self, x, recon_x, mu, logvar, z):\n",
    "        \"\"\"\n",
    "        Calcule la perte IWAE basée sur le Log-Sum-Exp.\n",
    "        L'objectif est de maximiser la borne : E[log( (1/K) * sum(p(x,z)/q(z|x)) )]\n",
    "        \"\"\"\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Répète x pour correspondre aux K échantillons de recon_x : (K, Batch, Input_size)\n",
    "        x_k = x.unsqueeze(0).repeat(self.K, 1, 1)\n",
    "\n",
    "        # 1. Log p(x | z) : Log-vraisemblance de la reconstruction (Log-Bernoulli)\n",
    "        # On somme sur la dimension des pixels (dim=2)\n",
    "        log_p_x_given_z = -F.binary_cross_entropy(recon_x, x_k, reduction=\"none\").sum(dim=2)\n",
    "\n",
    "        # 2. Log q(z | x) : Log-densité de la distribution de l'encodeur (Gaussienne)\n",
    "        # Formule : log N(z; mu, sigma^2)\n",
    "        log_q_z_given_x = -0.5 * (torch.log(2 * torch.tensor(np.pi)) + logvar + (z - mu).pow(2) / torch.exp(logvar))\n",
    "        log_q_z_given_x = log_q_z_given_x.sum(dim=2) # Somme sur les dimensions latentes -> (K, Batch)\n",
    "\n",
    "        # 3. Log p(z) : Log-densité de la priorité (Prior) N(0, 1)\n",
    "        log_p_z = -0.5 * (torch.log(2 * torch.tensor(np.pi)) + z.pow(2))\n",
    "        log_p_z = log_p_z.sum(dim=2) # Somme sur les dimensions latentes -> (K, Batch)\n",
    "\n",
    "        # --- CALCUL DES POIDS D'IMPORTANCE ---\n",
    "        # log_w = log( p(x,z) / q(z|x) ) = log p(x|z) + log p(z) - log q(z|x)\n",
    "        log_w = log_p_x_given_z + log_p_z - log_q_z_given_x\n",
    "\n",
    "        # --- LOG-SUM-EXP TRICK ---\n",
    "        # Pour éviter les instabilités numériques, on utilise logsumexp pour calculer le log de la moyenne\n",
    "        # loss = - Moyenne_sur_batch ( log ( (1/K) * sum(exp(log_w)) ) )\n",
    "        loss = - (torch.logsumexp(log_w, dim=0) - torch.log(torch.tensor(float(self.K)))).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4da25b",
   "metadata": {},
   "source": [
    "# IWAE vs VAE: Analysis Notebook\n",
    "\n",
    "This notebook aggregates the analysis results for the comparison between VAE (K=1), IWAE (K=5), and IWAE (K=20).\n",
    "\n",
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd2aa5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.abspath('.'))\n",
    "\n",
    "from src.analysis.evaluate_likelihood import evaluate_model\n",
    "\n",
    "# Config\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths\n",
    "vae_path = 'checkpoints/vae_k1_epochs50_seed42.pt'\n",
    "iwae_k5_path = 'checkpoints/iwae_k5_epochs50_seed42.pt'\n",
    "iwae_k20_path = 'checkpoints/iwae_k20_epochs50_seed42.pt'\n",
    "iwae_k50_path = 'checkpoints/iwae_k50_epochs50_seed42.pt'\n",
    "iwae_k100_path = 'checkpoints/iwae_k100_epochs50_seed42.pt'\n",
    "\n",
    "output_dir = './notebook_results'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183b39ba",
   "metadata": {},
   "source": [
    "## 2. Load Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b1d6f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE Loaded.\n",
      "IWAE (K=5) Loaded.\n",
      "IWAE (K=20) Loaded.\n",
      "IWAE (K=50) Loaded.\n",
      "IWAE (K=100) Loaded.\n",
      "Data Loaded: Train 50000, Val 10000, Test 10000\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_size = 200\n",
    "latent_size = 50\n",
    "output_size = 784\n",
    "\n",
    "# Load VAE\n",
    "vae = VAE(input_size, hidden_size, latent_size, output_size)\n",
    "vae.load_state_dict(torch.load(vae_path, map_location=device))\n",
    "vae.to(device)\n",
    "print(\"VAE Loaded.\")\n",
    "\n",
    "# Load IWAE K=5\n",
    "iwae_k5 = IWAE(5, input_size, hidden_size, latent_size, output_size)\n",
    "iwae_k5.load_state_dict(torch.load(iwae_k5_path, map_location=device))\n",
    "iwae_k5.to(device)\n",
    "print(\"IWAE (K=5) Loaded.\")\n",
    "\n",
    "# Load IWAE K=20\n",
    "iwae_k20 = IWAE(20, input_size, hidden_size, latent_size, output_size)\n",
    "iwae_k20.load_state_dict(torch.load(iwae_k20_path, map_location=device))\n",
    "iwae_k20.to(device)\n",
    "print(\"IWAE (K=20) Loaded.\")\n",
    "\n",
    "# Load IWAE K=50\n",
    "iwae_k50 = IWAE(50, input_size, hidden_size, latent_size, output_size)\n",
    "iwae_k50.load_state_dict(torch.load(iwae_k50_path, map_location=device))\n",
    "iwae_k50.to(device)\n",
    "print(\"IWAE (K=50) Loaded.\")\n",
    "\n",
    "# Load IWAE K=100\n",
    "iwae_k100 = IWAE(100, input_size, hidden_size, latent_size, output_size)\n",
    "iwae_k100.load_state_dict(torch.load(iwae_k100_path, map_location=device))\n",
    "iwae_k100.to(device)\n",
    "print(\"IWAE (K=100) Loaded.\")\n",
    "\n",
    "\n",
    "# Data\n",
    "train_loader, val_loader, test_loader = get_dataloaders(batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785dab92",
   "metadata": {},
   "source": [
    "## 3. Qualitative Comparison: Reconstructions & Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ead6f67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructions saved to ./notebook_results/vae_recon.png\n",
      "Samples saved to ./notebook_results/vae_samples.png\n",
      "Reconstructions saved to ./notebook_results/iwae_k5_recon.png\n",
      "Samples saved to ./notebook_results/iwae_k5_samples.png\n",
      "Reconstructions saved to ./notebook_results/iwae_k20_recon.png\n",
      "Samples saved to ./notebook_results/iwae_k20_samples.png\n",
      "Reconstructions saved to ./notebook_results/iwae_k50_recon.png\n",
      "Samples saved to ./notebook_results/iwae_k50_samples.png\n",
      "Reconstructions saved to ./notebook_results/iwae_k100_recon.png\n",
      "Samples saved to ./notebook_results/iwae_k100_samples.png\n",
      "Visualizations saved to ./notebook_results\n"
     ]
    }
   ],
   "source": [
    "# VAE Visuals\n",
    "plot_reconstruction(vae, test_loader, device, f\"{output_dir}/vae_recon.png\")\n",
    "plot_samples(vae, device, f\"{output_dir}/vae_samples.png\")\n",
    "\n",
    "# IWAE K=5 Visuals\n",
    "plot_reconstruction(iwae_k5, test_loader, device, f\"{output_dir}/iwae_k5_recon.png\")\n",
    "plot_samples(iwae_k5, device, f\"{output_dir}/iwae_k5_samples.png\")\n",
    "\n",
    "# IWAE K=20 Visuals\n",
    "plot_reconstruction(iwae_k20, test_loader, device, f\"{output_dir}/iwae_k20_recon.png\")\n",
    "plot_samples(iwae_k20, device, f\"{output_dir}/iwae_k20_samples.png\")\n",
    "\n",
    "# IWAE K=50 Visuals\n",
    "plot_reconstruction(iwae_k50, test_loader, device, f\"{output_dir}/iwae_k50_recon.png\")\n",
    "plot_samples(iwae_k50, device, f\"{output_dir}/iwae_k50_samples.png\")\n",
    "\n",
    "# IWAE K=100 Visuals\n",
    "plot_reconstruction(iwae_k100, test_loader, device, f\"{output_dir}/iwae_k100_recon.png\")\n",
    "plot_samples(iwae_k100, device, f\"{output_dir}/iwae_k100_samples.png\")\n",
    "\n",
    "# Display (if running in interactive notebook)\n",
    "print(\"Visualizations saved to ./notebook_results\")\n",
    "# plt.imshow(plt.imread(f\"{output_dir}/vae_recon.png\"))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1be4851",
   "metadata": {},
   "source": [
    "## 4. Quantitative Comparison: Log-Likelihood (IWAE bound, K=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba77417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating LL with K=5000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE LL: -81.0650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IWAE (K=5) LL: -78.3691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IWAE (K=20) LL: -77.3391\n",
      "Improvement (K=1->5): 2.6959 nats\n",
      "Improvement (K=5->20): 1.0300 nats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "k_eval = 5000\n",
    "print(f\"Estimating LL with K={k_eval}...\")\n",
    "\n",
    "# We strictly use IWAE logic for evaluation (even for VAE weights)\n",
    "evaluator = IWAE(k_eval, input_size, hidden_size, latent_size, output_size).to(device)\n",
    "\n",
    "# Evaluate VAE\n",
    "evaluator.load_state_dict(vae.state_dict())\n",
    "vae_ll = evaluate_model(evaluator, test_loader, device, k_eval)\n",
    "print(f\"VAE LL: {vae_ll:.4f}\")\n",
    "\n",
    "# Evaluate IWAE K=5\n",
    "evaluator.load_state_dict(iwae_k5.state_dict())\n",
    "iwae_k5_ll = evaluate_model(evaluator, test_loader, device, k_eval)\n",
    "print(f\"IWAE (K=5) LL: {iwae_k5_ll:.4f}\")\n",
    "\n",
    "# Evaluate IWAE K=20\n",
    "evaluator.load_state_dict(iwae_k20.state_dict())\n",
    "iwae_k20_ll = evaluate_model(evaluator, test_loader, device, k_eval)\n",
    "print(f\"IWAE (K=20) LL: {iwae_k20_ll:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8496a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improvement (K=1->5): 2.6959 nats\n",
      "Improvement (K=5->20): 1.0300 nats\n",
      "Improvement (K=1->20): 3.7259 nats\n"
     ]
    }
   ],
   "source": [
    "print(f\"Improvement (K=1->5): {iwae_k5_ll - vae_ll:.4f} nats\")\n",
    "print(f\"Improvement (K=5->20): {iwae_k20_ll - iwae_k5_ll:.4f} nats\")\n",
    "print(f\"Improvement (K=1->20): {iwae_k20_ll - vae_ll:.4f} nats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f52db8",
   "metadata": {},
   "source": [
    "## 5. Posterior Collapse Analysis: Effective KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aa31f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Active Units...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE Active Units: 15\n",
      "IWAE (K=5) Active Units: 22\n",
      "IWAE (K=20) Active Units: 26\n",
      "Plot saved to ./notebook_results/vae_kl.png\n",
      "Plot saved to ./notebook_results/iwae_k5_kl.png\n",
      "Plot saved to ./notebook_results/iwae_k20_kl.png\n"
     ]
    }
   ],
   "source": [
    "print(\"Analyzing Active Units...\")\n",
    "\n",
    "n_vae, vae_kls = calc_active_units(vae, test_loader, device)\n",
    "n_iwae_k5, iwae_k5_kls = calc_active_units(iwae_k5, test_loader, device)\n",
    "n_iwae_k20, iwae_k20_kls = calc_active_units(iwae_k20, test_loader, device)\n",
    "\n",
    "print(f\"VAE Active Units: {n_vae}\")\n",
    "print(f\"IWAE (K=5) Active Units: {n_iwae_k5}\")\n",
    "print(f\"IWAE (K=20) Active Units: {n_iwae_k20}\")\n",
    "\n",
    "plot_kl_stats(vae_kls, \"VAE\", f\"{output_dir}/vae_kl.png\")\n",
    "plot_kl_stats(iwae_k5_kls, \"IWAE K=5\", f\"{output_dir}/iwae_k5_kl.png\")\n",
    "plot_kl_stats(iwae_k20_kls, \"IWAE K=20\", f\"{output_dir}/iwae_k20_kl.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f6f2b7",
   "metadata": {},
   "source": [
    "## 6. Gradient Variance Analysis (SNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2024e826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Gradient SNR...\n",
      "Collecting gradients over 50 runs...\n",
      "VAE SNR: 0.5119\n",
      "Collecting gradients over 50 runs...\n",
      "IWAE (K=5) SNR: 0.2917\n",
      "Collecting gradients over 50 runs...\n",
      "IWAE (K=20) SNR: 0.2487\n"
     ]
    }
   ],
   "source": [
    "# Get a batch\n",
    "data, _ = next(iter(train_loader))\n",
    "data = data.to(device)\n",
    "\n",
    "print(\"Comparing Gradient SNR...\")\n",
    "\n",
    "var_vae, snr_vae = compute_gradient_variance(vae, data, n_runs=50)\n",
    "print(f\"VAE SNR: {snr_vae:.4f}\")\n",
    "\n",
    "var_iwae_k5, snr_iwae_k5 = compute_gradient_variance(iwae_k5, data, n_runs=50)\n",
    "print(f\"IWAE (K=5) SNR: {snr_iwae_k5:.4f}\")\n",
    "\n",
    "var_iwae_k20, snr_iwae_k20 = compute_gradient_variance(iwae_k20, data, n_runs=50)\n",
    "print(f\"IWAE (K=20) SNR: {snr_iwae_k20:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaidm-iwae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
